from model.mdp.mdp import MDP
from model.mdp.monte_carlo import MonteCarlo

if __name__ == "__main__":
    # 状态集合
    S = ["s1", "s2", "s3", "s4", "s5"]
    # 动作集合
    A = ["保持s1", "前往s1", "前往s2", "前往s3", "前往s4", "前往s5", "概率前往"]
    # 状态转移函数
    P = {
        ("s1", "保持s1", "s1"): 1.0,
        ("s1", "前往s2", "s2"): 1.0,
        ("s2", "前往s1", "s1"): 1.0,
        ("s2", "前往s3", "s3"): 1.0,
        ("s3", "前往s4", "s4"): 1.0,
        ("s3", "前往s5", "s5"): 1.0,
        ("s4", "前往s5", "s5"): 1.0,
        ("s4", "概率前往", "s2"): 0.2,
        ("s4", "概率前往", "s3"): 0.4,
        ("s4", "概率前往", "s4"): 0.4,
    }
    # 奖励函数
    R = {
        ("s1", "保持s1"): -1,
        ("s1", "前往s2"): 0,
        ("s2", "前往s1"): -1,
        ("s2", "前往s3"): -2,
        ("s3", "前往s4"): -2,
        ("s3", "前往s5"): 0,
        ("s4", "前往s5"): 10,
        ("s4", "概率前往"): 1,
    }
    # 折扣因子
    gamma = 0.5

    # 策略1，随机策略
    pi_1 = {
        "s1": {
            "保持s1": 0.5,
            "前往s2": 0.5
        },
        "s2": {
            "前往s1": 0.5,
            "前往s3": 0.5
        },
        "s3": {
            "前往s4": 0.5,
            "前往s5": 0.5
        },
        "s4": {
            "前往s5": 0.5,
            "概率前往": 0.5
        },
        "s5": {
        }
    }
    # 策略2
    pi_2 = {
        "s1": {
            "保持s1": 0.6,
            "前往s2": 0.4
        },
        "s2": {
            "前往s1": 0.3,
            "前往s3": 0.7
        },
        "s3": {
            "前往s4": 0.5,
            "前往s5": 0.5
        },
        "s4": {
            "前往s5": 0.1,
            "概率前往": 0.9
        },
        "s5": {
        }
    }
    mdp = MDP(S, A, P, R, gamma)
    mc = MonteCarlo(100, 100)
    mrp = mdp.to_mrp(pi_1)

    print(mc.compute_occupancy(mdp, pi_1, "s4", "概率前往", 0.5))
